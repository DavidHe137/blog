---
layout: post
title: "Everything I didn't know about VAEs (the first time around)"
date: 2024-05-27 10:57:48 -0400
categories: jekyll update
---

# Why another post on VAEs

There are already [many](https://arxiv.org/abs/1606.05908) [excellent](https://lilianweng.github.io/posts/2018-08-12-vae/) blog posts and videos on VAEs. However, I had a hard time understanding some of the design decisions for the VAE framework, namely the reasons for using latent variables and variational inference. In this post, I hope to clearly explain all the questions I had while learning this topic.

# Preliminaries/Notation/Terms to Define

**TODO**:

- Bayes rule
- Integrals
- Bayesian inference
- variational inference

**Confusions**:

- what is inference and learning?
- what is a model? is it a distribution with parameters? or an neural network?
- What is a distribution? What are the inputs and outputs?

- A distribution has a probability density function (PDF), typically p(x) that takes an input example x and outputs a probability (a number between 0 and 1). However, a distribution and a pdf are not the same thing?

# The problem: approximating p(x)

We are given a dataset X generated by drawing examples from some distribution p(x).

For example, p(x) reflects some underlying data generating process [tran]. Our goal is to try and create a model to approximate p(x). _This is not the same as memorizing the examples_ Notice that we aren't necessarily trying to "learn the model" (at least not yet), as learning (-yeah what is learning?) is not the only way to approximate models from data.

## Making things concrete

What is p(x). Why do we care about approximating p(x)?
_what does it mean to approximate a distribution. What is the final concrete object that we want. It turns out that we want an object where we can sample examples according to the distribution, as well as evluate likelihood, inpaint_

- Generative Modeling
- Denoising/Inpainting

# A solution: MLE: A standard approach to approximating p(x)

-- do we assume our model's form before MLE?
A standard approach for approximating p(x) is Maximum Likelihood Estimation (MLE), which involves finding the parameters of a model.
_This is not the only approach to approximating p(x), as other common approaches include MCMC, score-matching models, energy, flows, autoregressive. I hope to cover these in future posts._

<details>
  <summary>Why do we optimize the log?</summary>
  1. it is the same as the original
  2. it makes math easier
    1. floating point precision (mutlutplicaitons become sums)
    2. expressions (we dont' have to evaluate the exponential for gaussians)

</details>

Some popular ways of maximing MLE include computing the exact solution,
expectation maximization, CAVI? Some of these

- computing the exact solution?
  --(when can we do this?)

<details>
  <summary>An example of Maximum Likelihood</summary>
  #TODO:

</details>

- optimizing model parameters with respect to the ELBO
- why variational inference (and amortized inference)?

-- The MLE objective has a degenerate solution, so it's not the final goal. How do we make sure this proxy objective gets us what we actually want?

_we can view ML as minimizing KL. If the dataset has a Dirac-Delta distribution, why don't we end up here?_

<details>
  <summary>Miniziming the KL leads to minimizng NLL</summary>
  #TODO:

</details>

# Our model and latents

This is all fine and good, but let's say we wanted to incoporate some prior knowledge into our model. To illustrate, CS 228 gives the example of modeling data that comes from a mixture of Gaussians. If we used a single Gaussian, we would have a hard time fitting the data well.

This is why we introduce latent variables z. **We can view continuous latents as infinite mixtures**.
_what's the harm of choosing a single extremely flexible family class of models?_

Our latents do not have to be this strict. On a more abstract level, we can imagine news articles originating from a topic.

As an example, see Blei

# Challenges: Intractability

While latents make the modeling easier, it also present a problem for calculating p(x) when we are working with

- high dimensional outputs (are high-dimensional inputs hard to work with?)
- large datasets

Introduicin p(x) = p(x|z)p(z) makes p(x) intractable, or very difficult to compute. To revisit our example of a mixture of Gaussians [Blei].
_this wouldn't be a problem if we didn't have latents? refer to previous MLE example?_

More generally,
why is the integral intractable? why no analytical solution? why not computationally efficient?
_what does intractable mean. does it mean that it's impossible to compute, or practically impossible to compute?_

Since we've decided that this is intractable, is there a way we can get compute something that is at least somewhat close? The answer is yes!

# Variational Inference

_is variational inference about optimizng distributions with KL, or approximating the posterior?_

## The ELBO [Calvin Luo]

$$
p(x) = \frac{p(x, z)}{p(z|x)}
$$

_how do we calculate the joint?_
We don't know how to calculate $p(z|x)$ (known as the posterior) because _TODO_.

Another way to see this intractability is to write

$$
p(x) = \int_z p(x, z) dz
$$

_does this require us to integrate over all z?_

However, we may choose to select a model family and optimize parameters to
_variational inference photo_. This is referred to as variational becaues of _TODO, variational calculus?_. I explain later how this is the variational part of the vae.

**Key Idea Number 1: VI/ELBO: p(z|x) is intractable, so let's replace it by optimizing parameters of an approximating distribution**

<!--
$$
\begin{align*}
  \log p(x)
    &= \int_{z\sim q(z|x)} \log p(x)\\ && \text{_we do this because we would like to incorporate our approximating distirbution into our caluclation of p(x). We can do this because integrating over a probability distribution is one}._
    &= \int_{z\sim q(z|x)} \log p(x) \\
    &=
\end{align*}
$$ -->

Now I want to talk specifically about the approach of optimizing the ELBO, as this is the Variational Inference aspect of VAEs.

This is not the only way you could have arrived here. For one you could have chosen to find the lower bound of the log likelihood with with Jensen's Inequality, but it's a less insightful derivation. You could also see this by trying to minimize the KL (math here), which is described here, but I encourage you to do with pen and paper as a useful exercise.

## Issues with the ELBO

we can't differentiate
-- q gradient product rule
https://ermongroup.github.io/cs228-notes/extras/vae/

we can try to estimate this with high variance whatever

# The reparametrization trick

This is (one of) the main contribution of the VAE paper

deterministic of x and phi
From (some source), this is about externalizing randomness.
-- TODO: do we lose anything by using the reparametrization trick? or does it just let us approximate?

These approaches

## Implementing VAEs in practice

We'll move on from our example of a mixture of Gaussians to the more difficult of generating new images.

# Neural Networks

Notice how I haven't mentioned Neural Networks yet.

# Monte-Carlo

when is Monte Carlo estimation ok and not ok?
how big do mini-bactches need to be

# Choosing model families and parameters

flexible distributions, expressive models for parameters?

# References

I encourage you to read Durk Kingma's paper, thesis, or tutorial. His writing is clear.
Please let me know of any errors.

Carl Doersch
Calvin Luo
Kevin Frans
Lilian Weng https://lilianweng.github.io/posts/2018-08-12-vae/
tran https://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models
https://arxiv.org/pdf/1906.02691
https://arxiv.org/pdf/1312.6114
Blei https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf (has MLE and KL derivations for ELBO)
https://ermongroup.github.io/cs228-notes/learning/latent/ (why latents)
https://www.youtube.com/watch?v=vJo7hiMxbQ8
https://www.youtube.com/watch?v=c27SHdQr4lw
https://www.youtube.com/watch?v=3G5hWM6jqPk
gradient estimator
228
https://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-14.pdf
https://bochang.me/blog/posts/measure-val-grad/#:~:text=The%20score%20function%20estimator%20is,(x%3BÎ¸)%5D.

expectation maximization
http://stillbreeze.github.io/Variational-Inference-and-Expectation-Maximization/

## How do we calculate p(x, z)

## What is inference

- ok so we are trying to find the parameters of q(x|z).
- Are we also trying to perform "inference" on p(x)?
